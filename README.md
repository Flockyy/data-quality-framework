# Data Quality Framework ğŸ¯

A comprehensive, production-ready framework for automated data quality validation, profiling, and monitoring. Built to ensure data reliability across your data pipelines and warehouses.

## ğŸš€ Features

### Data Profiling
- **Automatic statistical analysis** - Mean, median, std deviation, quartiles
- **Distribution analysis** - Detect skewness, outliers, and patterns
- **Data type inference** - Automatic schema detection
- **Missing data analysis** - Identify null patterns and completeness issues
- **Cardinality checks** - Unique values, duplicates detection

### Data Validation
- **Rule-based validation** - Custom validation rules with SQL-like syntax
- **Great Expectations integration** - Industry-standard expectations library
- **Schema validation** - Ensure data conforms to expected structure
- **Reference data checks** - Validate against lookup tables
- **Cross-field validation** - Complex multi-column rules
- **Custom validators** - Extensible validation framework

### Monitoring & Alerting
- **Real-time quality metrics** - Track data quality over time
- **Anomaly detection** - Identify unusual patterns automatically
- **Alert system** - Email, Slack, webhook notifications
- **Quality dashboards** - Interactive visualizations
- **Historical trending** - Track quality improvements/degradations
- **SLA monitoring** - Set and track data quality SLAs

### Reporting
- **HTML reports** - Detailed, shareable quality reports
- **JSON exports** - Machine-readable results
- **Data lineage tracking** - Understand data flow and impact
- **Executive summaries** - High-level quality scorecards

## ğŸ“‹ Prerequisites

- Python 3.9+
- PostgreSQL (optional, for metadata storage)
- Redis (optional, for caching)

## ğŸ”§ Installation

### Quick Start with UV (Recommended - 10x Faster!)

```bash
# Install UV (fast Python package installer)
curl -LsSf https://astral.sh/uv/install.sh | sh  # macOS/Linux
# Or: pip install uv

# Clone the repository
git clone https://github.com/Flockyy/data-quality-framework.git
cd data-quality-framework

# Install with all dev tools (uses uv for speed)
make dev-setup

# Or manually:
uv pip install -e ".[dev]"
pre-commit install
```

### Traditional Installation

```bash
# Clone the repository
git clone https://github.com/Flockyy/data-quality-framework.git
cd data-quality-framework

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Install the package in development mode
pip install -e .
```

## ğŸ¯ Quick Start

### 1. Basic Data Profiling

```python
from dqf import DataProfiler
import pandas as pd

# Load your data
df = pd.read_csv('data/sales.csv')

# Create profiler
profiler = DataProfiler()

# Generate profile
profile = profiler.profile(df, dataset_name="sales_data")

# View results
print(profile.summary())

# Export report
profile.to_html('reports/sales_profile.html')
```

### 2. Data Validation

```python
from dqf import DataValidator, ValidationRule

# Create validator
validator = DataValidator()

# Define rules
rules = [
    ValidationRule("email", "is_email", "Email must be valid"),
    ValidationRule("age", "between", "Age must be 18-100", min=18, max=100),
    ValidationRule("price", "greater_than", "Price must be positive", value=0),
    ValidationRule(["city", "country"], "not_null", "Location required"),
]

# Validate data
results = validator.validate(df, rules)

# Check results
if results.is_valid:
    print("âœ… All validations passed!")
else:
    print(f"âŒ {results.failure_count} validations failed")
    print(results.get_failures())
```

### 3. Using Configuration Files

```python
from dqf import DQFramework

# Initialize from config
dqf = DQFramework.from_config('config/data_quality_config.yaml')

# Run complete quality check
results = dqf.run_quality_check(
    data_source='postgres://localhost/sales_db',
    dataset='orders',
    profile=True,
    validate=True,
    monitor=True
)

# Generate comprehensive report
results.generate_report('reports/orders_quality_report.html')
```

## ğŸ“ Project Structure

```
data-quality-framework/
â”œâ”€â”€ dqf/                          # Main package
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ profiler.py              # Data profiling engine
â”‚   â”œâ”€â”€ validator.py             # Validation engine
â”‚   â”œâ”€â”€ monitor.py               # Monitoring & alerting
â”‚   â”œâ”€â”€ reporters.py             # Report generation
â”‚   â”œâ”€â”€ rules/                   # Validation rules
â”‚   â”‚   â”œâ”€â”€ base.py
â”‚   â”‚   â”œâ”€â”€ builtin_rules.py
â”‚   â”‚   â””â”€â”€ custom_rules.py
â”‚   â”œâ”€â”€ utils/                   # Utilities
â”‚   â”‚   â”œâ”€â”€ database.py
â”‚   â”‚   â”œâ”€â”€ metrics.py
â”‚   â”‚   â””â”€â”€ notifications.py
â”‚   â””â”€â”€ integrations/            # Third-party integrations
â”‚       â”œâ”€â”€ great_expectations.py
â”‚       â”œâ”€â”€ dbt.py
â”‚       â””â”€â”€ airflow.py
â”œâ”€â”€ config/                       # Configuration files
â”‚   â”œâ”€â”€ data_quality_config.yaml
â”‚   â””â”€â”€ alerting_config.yaml
â”œâ”€â”€ examples/                     # Example implementations
â”‚   â”œâ”€â”€ basic_profiling.py
â”‚   â”œâ”€â”€ custom_validators.py
â”‚   â”œâ”€â”€ pipeline_integration.py
â”‚   â””â”€â”€ dashboard_example.py
â”œâ”€â”€ tests/                        # Test suite
â”‚   â”œâ”€â”€ test_profiler.py
â”‚   â”œâ”€â”€ test_validator.py
â”‚   â””â”€â”€ test_monitor.py
â”œâ”€â”€ data/                         # Sample datasets
â”‚   â”œâ”€â”€ customers.csv
â”‚   â”œâ”€â”€ transactions.csv
â”‚   â””â”€â”€ products.csv
â”œâ”€â”€ reports/                      # Generated reports
â”œâ”€â”€ dashboard/                    # Streamlit dashboard
â”‚   â””â”€â”€ app.py
â”œâ”€â”€ docker/                       # Docker setup
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â””â”€â”€ README.md
```

## ğŸ” Advanced Usage

### Custom Validation Rules

```python
from dqf.rules import BaseRule

class CustomBusinessRule(BaseRule):
    """Validate business-specific logic"""

    def validate(self, df):
        # Custom validation logic
        invalid = df[df['revenue'] < df['cost']]

        return {
            'valid': len(invalid) == 0,
            'failures': len(invalid),
            'details': invalid.to_dict('records')
        }

# Register and use
validator.register_rule('business_logic', CustomBusinessRule())
```

### Integration with Data Pipelines

```python
# Airflow DAG example
from airflow import DAG
from dqf.integrations.airflow import DataQualityOperator

dag = DAG('etl_with_quality_checks', ...)

quality_check = DataQualityOperator(
    task_id='validate_staging_data',
    config_file='config/staging_quality.yaml',
    fail_on_error=True,
    dag=dag
)
```

### Monitoring Dashboard

```bash
# Launch interactive dashboard
streamlit run dashboard/app.py

# Access at http://localhost:8501
```

## ğŸ“Š Configuration Example

```yaml
# config/data_quality_config.yaml
profiling:
  enabled: true
  statistics: true
  distributions: true
  correlations: true

validation:
  enabled: true
  rules:
    - column: customer_id
      type: not_null
      severity: critical

    - column: email
      type: regex
      pattern: '^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
      severity: high

    - column: amount
      type: range
      min: 0
      max: 1000000
      severity: medium

monitoring:
  enabled: true
  metrics:
    - completeness
    - uniqueness
    - validity
    - consistency

  alerts:
    - condition: completeness < 0.95
      channel: slack
      severity: high

    - condition: validity < 0.99
      channel: email
      recipients: [data-team@company.com]

reporting:
  format: html
  schedule: daily
  recipients: [team@company.com]
```

## ğŸ§ª Testing

```bash
# Run tests (using pytest)
make test

# With coverage report
make test-cov

# Run tests in parallel (faster)
make test-parallel

# Run specific test
pytest tests/test_validator.py -k "test_email_validation" -v

# Run only unit tests
pytest tests/ -m unit

# Skip slow tests
pytest tests/ -m "not slow"
```

## ğŸ› ï¸ Modern Development Tools

This project uses cutting-edge Python tooling for better performance and developer experience:

- **UV** - 10-100x faster than pip for package installation
- **Ruff** - Lightning-fast linter and formatter (replaces Black, Flake8, isort)
- **Pre-commit** - Automated code quality checks before each commit
- **Pytest** - Comprehensive test coverage with parallel execution

See [SETUP_GUIDE.md](SETUP_GUIDE.md) for detailed setup instructions.

### Quick Commands

```bash
# Code quality checks
make lint           # Run ruff linter
make format         # Format code with ruff
make typecheck      # Run mypy type checker
make check          # Run all checks (like CI)

# Development
make dev-setup      # Set up complete dev environment
make clean          # Clean build artifacts
```

## ğŸ³ Docker Deployment

```bash
# Build and run with Docker Compose
docker-compose up -d

# Access dashboard
open http://localhost:8501

# View logs
docker-compose logs -f
```

## ğŸ“ Use Cases

1. **ETL Pipeline Validation** - Validate data at each stage of transformation
2. **Data Warehouse Quality** - Continuous monitoring of warehouse tables
3. **ML Feature Validation** - Ensure training data quality
4. **API Data Validation** - Validate external data sources
5. **Regulatory Compliance** - Document and prove data quality
6. **Data Migration** - Validate before/after migration

## ğŸ“ˆ Metrics & KPIs

The framework tracks key data quality dimensions:

- **Completeness**: % of non-null values
- **Uniqueness**: % of unique values
- **Validity**: % passing validation rules
- **Consistency**: Cross-field consistency checks
- **Accuracy**: Comparison with reference data
- **Timeliness**: Data freshness checks

## ğŸ¤ Contributing

Contributions welcome! Please read [CONTRIBUTING.md](CONTRIBUTING.md) for details.

## ğŸ“ License

MIT License - see [LICENSE](LICENSE) file

## ğŸ”— Related Projects

- [Great Expectations](https://greatexpectations.io/)
- [Pandera](https://pandera.readthedocs.io/)
- [dbt](https://www.getdbt.com/)

## ğŸ“§ Contact

- GitHub: [@Flockyy](https://github.com/Flockyy)
- LinkedIn: [Your Profile](https://linkedin.com/in/yourprofile)

---

**Built with â¤ï¸ for reliable data engineering**
